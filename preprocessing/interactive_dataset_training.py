#!/usr/bin/env python
__author__ = "Mohamed Adnen Abdessaied"
__version__ = "1.0"
__maintainer__ = "Mohamed Adnen Abdessaied"
__email__ = "adnenabdessayed@gmail.com"
__status__ = "Implementation"

"""
This class implements a dataset that will be used afterwards in the interactive mode as the data structures will be 
different. New data will be generated by a user based on how satisfied he/she is with the predictions of a pre-trained
model. These data will be used for fine-tuning.
"""

from preprocessing.interactive_dataset import Interactive_Dataset
from preprocessing.statics import (
    CAPTION,
    DIALOG,
    ANSWER,
    QUESTIONS,
    QUESTION,
    HISTORY,
    IMG_FEATURES,
    ANSWERS_GEN_TR,
    ANSWERS_GEN_TARGETS,
    SOS_TOKEN,
    EOS_TOKEN,
)
from preprocessing.extract_image_features import ExtractImageFeatures


class Interactive_Dataset_Training(Interactive_Dataset):
    def __init__(self,
                 gui_data,
                 params: dict,
                 saved_word_to_idx_path,
                 add_boundary_toks: bool = True,
                 concatenate_history: bool = True,
                 feature_extractor: str = "VGG",
                 ):
        """
        Class constructor.
        :param gui_data: data collected from the interactive mode.
        :param params: Dict containing all the (hyper) parameters of the experiment.
        :param saved_word_to_idx_path: Path to the saved word to index mapping.
        :param concatenate_history: If true, the history rounds will be concatenated at each time step
        """
        super(Interactive_Dataset_Training, self).__init__(
            gui_data,
            params,
            saved_word_to_idx_path,
            add_boundary_toks=add_boundary_toks,
            concatenate_history=concatenate_history,
            feature_extractor=feature_extractor)
        self.img_paths = list(self.gui_data.keys())

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx, image_transform=ExtractImageFeatures((224, 224))):
        image_path = self.img_paths[idx]
        dialog = self.gui_data[image_path][DIALOG]

        caption = self.from_word_to_idx(self.preprocess_string(self.gui_data[image_path][CAPTION]))

        for i in range(len(dialog)):
            question = self.preprocess_string(dialog[i][QUESTION])
            answer = self.preprocess_string(dialog[i][ANSWER])

            dialog[i][QUESTION] = self.from_word_to_idx(question)
            if self.add_boundary_toks:
                dialog[i][ANSWER] = self.from_word_to_idx([SOS_TOKEN] + answer + [EOS_TOKEN])
            else:
                dialog[i][ANSWER] = self.from_word_to_idx(answer)

        questions = self._pad_sequences([dialog_round[QUESTION] for dialog_round in dialog])
        history = self._get_history(caption,
                                    [dialog_round[QUESTION] for dialog_round in dialog],
                                    [dialog_round[QUESTION] for dialog_round in dialog],
                                    concatenate_history=self.concatenate_history)

        answers_in = self._pad_sequences([dialog_round[ANSWER][:-1] for dialog_round in dialog])
        answers_out = self._pad_sequences([dialog_round[ANSWER][1:] for dialog_round in dialog])

        item = {IMG_FEATURES: image_transform(image_path, self.feature_extactor),
                QUESTIONS: questions.long(),
                HISTORY: history.long(),
                ANSWERS_GEN_TR: answers_in.long(),
                ANSWERS_GEN_TARGETS: answers_out.long(),
                }

        return item, image_path
