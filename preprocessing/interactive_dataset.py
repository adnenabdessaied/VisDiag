#!/usr/bin/env python
__author__ = "Mohamed Adnen Abdessaied"
__version__ = "1.0"
__maintainer__ = "Mohamed Adnen Abdessaied"
__email__ = "adnenabdessayed@gmail.com"
__status__ = "Implementation"

"""
This class implements a dataset that will be used afterwards in the interactive mode as the data structures will be 
different. New data will be generated by a user based on how satisfied he/she is with the predictions of a pre-trained
model. These data will be used for fine-tuning.
"""

import os
import pickle
import torch
from torch.utils.data.dataset import Dataset
from torch.nn.utils.rnn import pad_sequence
from feature_extractors.custom_nets import VGG_clipped, Googlenet_clipped
from nltk.tokenize import word_tokenize
import string
from num2words import num2words
from preprocessing.contractions import CONTRACTIONS_DICT
from preprocessing.statics import (
    MAX_SENTENCE_LENGTH,
    PAD_INDEX,
    UNK_INDEX
)
from preprocessing.extract_image_features import ExtractImageFeatures


class Interactive_Dataset(Dataset):
    def __init__(self,
                 gui_data,
                 params: dict,
                 saved_word_to_idx_path,
                 add_boundary_toks: bool = True,
                 concatenate_history: bool = True,
                 feature_extractor: str = "VGG",
                 ):
        """
        Class constructor.
        :param gui_data: data collected from the interactive mode.
        :param params: Dict containing all the (hyper) parameters of the experiment.
        :param saved_word_to_idx_path: Path to the saved word to index mapping.
        :param concatenate_history: If true, the history rounds will be concatenated at each time step
        :param feature_extractor: The name of the feature extractor to be used
        """
        super(Interactive_Dataset, self).__init__()
        self.gui_data = gui_data
        self.add_boundary_toks = add_boundary_toks
        self.params = params
        assert feature_extractor in ["VGG", "Googlenet"]
        if feature_extractor == "VGG":
            self.feature_extactor = VGG_clipped().eval()
        else:
            self.feature_extactor = Googlenet_clipped().eval()

        assert os.path.isfile(saved_word_to_idx_path), "There is no file under the path: {}".format(
            saved_word_to_idx_path)
        with open(saved_word_to_idx_path, "rb") as f:
            self.word_to_idx = pickle.load(f)
        self.concatenate_history = concatenate_history

    def __len__(self):
        raise NotImplementedError

    def __getitem__(self, idx, image_transform=ExtractImageFeatures((224, 224))):
        raise NotImplementedError

    def _pad_sequences(self, sequences):
        """Given tokenized sequences (either questions, answers or answer
        options, tokenized in ``__getitem__``), padding them to maximum
        specified sequence length. Return as a tensor of size
        ``(*, max_sequence_length)``.
        This method is only called in ``__getitem__``, chunked out separately
        for readability.
        Parameters
        ----------
        sequences : List[List[int]]
            List of tokenized sequences, each sequence is typically a
            List[int].
        Returns
        -------
        torch.Tensor, torch.Tensor
            Tensor of sequences padded to max length, and length of sequences
            before padding.
        """

        for i in range(len(sequences)):
            sequences[i] = sequences[i][: self.params[MAX_SENTENCE_LENGTH]]

        # Pad all sequences to max_sequence_length.
        maxpadded_sequences = torch.full((len(sequences), self.params[MAX_SENTENCE_LENGTH]), fill_value=PAD_INDEX)
        padded_sequences = pad_sequence(
            [torch.tensor(sequence) for sequence in sequences],
            batch_first=True,
            padding_value=PAD_INDEX
        )
        maxpadded_sequences[:, : padded_sequences.size(1)] = padded_sequences
        return maxpadded_sequences

    def _get_history(self, caption, questions, answers, concatenate_history: bool = False):
        """
        A function that generates the dialog history based on the image_id.
        :param image_id: id of the image in question.
        :param concatenate_history: A flag used to differentiate between two generation modes.
        :return: Dialog history H and dialog_history_length H_lengths as dictionaries.
            If concatenate_history == False: H = [C, [Q_1, A_1], [Q_2, A_2], ..., [Q_9, A_9]]
            else: H = {H_0: [C], H_1: [C, Q_1, A_1], H_2:[C, Q_1, A_1, Q_2, A_2], ... ,
                       H_9: [C, Q_1, A_1, ..., Q_9, A_9]}
            H_i is not a list but a torch.tensor!!
        """
        # treat caption as a qa-pair --> double the max length
        caption = caption[: self.params[MAX_SENTENCE_LENGTH] * 2]
        history = [caption]
        # Trim the questions and the answers
        for i in range(len(questions)):
            questions[i] = questions[i][: self.params[MAX_SENTENCE_LENGTH]]

        for i in range(len(answers)):
            answers[i] = answers[i][: self.params[MAX_SENTENCE_LENGTH]]

        for (q, a) in zip(questions, answers):
            history.append(q + a)

        # The last q-a pair is not necessary as we do not have 11 dialog rounds.
        history = history[:-1]

        max_length = 2 * self.params[MAX_SENTENCE_LENGTH]

        if concatenate_history:
            max_length = 2 * self.params[MAX_SENTENCE_LENGTH] * len(history)
            concatenated_history = []
            concatenated_history.append(caption)
            for i in range(1, len(history)):
                concatenated_history.append([])
                for j in range(1, i + 1):
                    concatenated_history[i].extend(history[j])
            history = concatenated_history
        padded_history_max = torch.full((len(history), max_length), fill_value=PAD_INDEX)
        padded_history = pad_sequence([torch.tensor(_round) for _round in history], batch_first=True,
                                      padding_value=PAD_INDEX)
        padded_history_max[:, : padded_history.size(1)] = padded_history
        return padded_history_max

    @staticmethod
    def preprocess_string(input_string: str):
        """
        Method that pre-processes an input string:
          lower-casing -> replacing numbers with letters + removing contractions + removing punctuations -> tokenizing
          -> removing stop words.
        :param input_string: The string to be pre-processed.
        :return: (list): tokenized, pre-processed string.
    """
        if input_string == "":
            return [""]
        else:
            punc = string.punctuation
            punc = "".join(c for c in punc if c is not "\'")
            input_string = input_string.replace("-", " ")
            input_string = "".join([c for c in input_string if c not in punc])
            input_string = input_string.lower()
            input_string = input_string.split(" ")
            input_string = [CONTRACTIONS_DICT[word] if word in CONTRACTIONS_DICT else word for word in input_string]
            input_string = list(map(lambda x: x.replace("\'", " "), input_string))

            for i, word in enumerate(input_string):
                input_string[i] = "".join([c for c in word if c not in punc])

            input_string = word_tokenize(" ".join(input_string))

            for i, word in enumerate(input_string):
                if word.isdigit():
                    num_str = num2words(word)
                    num_str = "".join([c for c in num_str if c not in punc])
                    num_str = word_tokenize(num_str)
                    input_string[i:i] = num_str
                    input_string.remove(word)
            return input_string

    def from_word_to_idx(self, input_string: list):
        """
        Performs a mapping from words to indices.
        :param input_string: the input string after pre-processing, e.g. input_string = [word_1, word_2, ..., word_n]
        :return: list of indices w.r.t. a vocabulary of the words contained in input_string
        """
        assert isinstance(input_string, list)
        input_indices = []
        for word in input_string:
            if word.isdigit():
                word = num2words(word)
            input_indices.append(self.word_to_idx.get(word, UNK_INDEX))
        return input_indices
